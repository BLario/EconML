{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intrumental Variables Applications Example\n",
    "\n",
    "### Summary of Contents:\n",
    "1. [Introduction](#intro)\n",
    "2. [NLSYM Dataset](#data)\n",
    "3. [A Gentle Start: The Naive Approach](#naive)\n",
    "4. [Using Instrumental Variables: 2SLS](#2sls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "To measure true causal effects of a treatment $T$ on an outcome $Y$ from observational data, we need to record all features $X$ that might influence both $T$ and $Y$. These $X$'s are called confounders. \n",
    "\n",
    "When some confounders are not recorded in the data, we might get biased estimates of the treatment effect. Here is an example:\n",
    "* Children of high-income parents might attain higher levels of education (e.g. college) since they can afford it\n",
    "* Children of high-income parents might also obtain better paying jobs due to parents' connections and knowledge\n",
    "* At first sight, it might appear as if education has an effect on income, when in fact this could be fully explained by family background\n",
    "\n",
    "There are several reasons for not recording all possible confounders, such as incomplete data or a confounder that is difficult to quantify (e.g. parental involvement). However, not all is lost! In cases such as these, we can use instrumental variables $Z$, features that affect the outcome only through their effect on the treatment. \n",
    "\n",
    "In this notebook, we use a real-world problem to show how treatment effects can be extracted with the help of instrumental variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NLSYM Dataset <a class=\"anchor\" id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://straubroland.files.wordpress.com/2010/12/education_technology-resized-600.png\" width=400px/>\n",
    "\n",
    "Describe the dataset briefly:\n",
    "\n",
    "* who collected it\n",
    "* method of collection \n",
    "* feature description \n",
    "* what we are interested in (average treatment effect as well as features of heterogeneity)\n",
    "\n",
    "The world can then be modelled as:\n",
    "$$\n",
    "\\begin{align}\n",
    "Y & = \\theta(X) \\cdot T + f(W) + \\epsilon\\\\\n",
    "T & = g(Z) + \\eta\n",
    "\\end{align}\n",
    "$$\n",
    "where $Y$ - outcome of interest, $X$ - features of heterogeneity, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports\n",
    "from econml.two_stage_least_squares import NonparametricTwoStageLeastSquares\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "df = pd.read_csv(\"data/card.csv\", dtype=float)\n",
    "data_filter = df['educ'].values >= 6\n",
    "T = df['educ'].values[data_filter]\n",
    "Z = df['nearc4'].values[data_filter]\n",
    "Y = df['lwage'].values[data_filter]\n",
    "\n",
    "# Impute missing values with mean, add dummy columns\n",
    "# Filter outliers (interviewees with less than 6 years of education)\n",
    "X_df = df[['exper', 'expersq']].copy()\n",
    "X_df['fatheduc'] = df['fatheduc'].fillna(value=df['fatheduc'].mean())\n",
    "X_df['fatheduc_nan'] = df['fatheduc'].isnull() * 1\n",
    "X_df['motheduc'] = df['motheduc'].fillna(value=df['motheduc'].mean())\n",
    "X_df['motheduc_nan'] = df['motheduc'].isnull() * 1\n",
    "X_df[['momdad14', 'sinmom14', 'reg661', 'reg662',\n",
    "        'reg663', 'reg664', 'reg665', 'reg666', 'reg667', 'reg668', 'reg669', 'south66']] = df[['momdad14', 'sinmom14', \n",
    "        'reg661', 'reg662','reg663', 'reg664', 'reg665', 'reg666', 'reg667', 'reg668', 'reg669', 'south66']]\n",
    "X_df[['black', 'smsa', 'south', 'smsa66']] = df[['black', 'smsa', 'south', 'smsa66']]\n",
    "columns_to_scale = ['fatheduc', 'motheduc', 'exper', 'expersq']\n",
    "# Scale continuous variables\n",
    "scaler = StandardScaler()\n",
    "X_df[columns_to_scale] = scaler.fit_transform(X_df[columns_to_scale])\n",
    "X = X_df.values[data_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. A Gentle Start: The Naive Approach <a class=\"anchor\" id=\"naive\"></a>\n",
    "\n",
    "Let's assume we know nothing about instrumental variables and we want to measure the treatment effect of schooling on wages. We can apply something like DML to do this and extract a treatment effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.dml import DMLCateEstimator\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_est = DMLCateEstimator(model_y=RandomForestRegressor(n_estimators=100), model_t=RandomForestRegressor(n_estimators=100))\n",
    "dml_est.fit(Y, T, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07118106872292643"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dml_est.effect(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06450203, -0.02916734,  0.03304918,  0.00789729,  0.01770636,\n",
       "       -0.00051149, -0.0255208 , -0.0143534 , -0.04458493,  0.0079625 ,\n",
       "        0.02696753, -0.00425429, -0.01666799,  0.00619979,  0.00258502,\n",
       "       -0.00454056,  0.02402749,  0.02222253,  0.00424424,  0.00192012,\n",
       "        0.01053959,  0.02104726, -0.00477149])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dml_est._model_final.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['exper', 'expersq', 'fatheduc', 'fatheduc_nan', 'motheduc',\n",
       "       'motheduc_nan', 'momdad14', 'sinmom14', 'reg661', 'reg662', 'reg663',\n",
       "       'reg664', 'reg665', 'reg666', 'reg667', 'reg668', 'reg669', 'south66',\n",
       "       'black', 'smsa', 'south', 'smsa66'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using Intrumental Variables: 2SLS <a class=\"anchor\" id=\"2sls\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = X\n",
    "Z = Z.reshape(-1, 1)\n",
    "T = T.reshape(-1, 1)\n",
    "X = np.ones_like(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_sls_est = NonparametricTwoStageLeastSquares(\n",
    "    t_featurizer=PolynomialFeatures(degree=1, include_bias=False),\n",
    "    x_featurizer=PolynomialFeatures(degree=1, include_bias=False),\n",
    "    z_featurizer=PolynomialFeatures(degree=1, include_bias=False),\n",
    "    dt_featurizer=None) # dt_featurizer only matters for marginal_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<econml.two_stage_least_squares.NonparametricTwoStageLeastSquares at 0x1ffab2b1ac8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sls_est.fit(Y, T, X, W, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13422248])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sls_est.effect(np.ones((1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
